# Agent

<small>Everything below is generated by an LLM. I take no responsibility for any of it, unless it's awesome... then it was pure prompting skills which I will take credit for.</small>

---

A small Ruby wrapper around [RubyLLM](https://github.com/alexrudall/ruby_llm) that provides custom tools and simplified methods for scripting with LLMs.

**Note:** This project is primarily for my personal use, but you might find it useful if you're looking for a simple way to script interactions with Claude via AWS Bedrock.

## Overview

Agent provides a clean interface for working with LLMs in Ruby scripts, with built-in support for:

- **Extended chat methods** - Get responses without boilerplate
- **Automatic query persistence** - All interactions saved to SQLite
- **Cost tracking** - Detailed reports on token usage and costs
- **Custom tools** - File operations, grep, Rails tests, and more
- **Schema validation** - RubyLLM Schema support for structured responses

## Installation

This gem is not pushed to rubygems. Instead, you should add a git reference to
your Gemfile.

## Quick Start

```ruby
require 'agent_pk'

# Configure the agent
AgentPk.configure do |config|
  config.db_path = 'tmp/db/agent.sqlite3'
  config.project = 'my_project'
  config.workspace_dir = Dir.pwd
end

# Start a chat
chat = AgentPk::Chat.new

# Simple question
response = chat.ask("What is the capital of France?")
```

## Key Features

### Extended Chat Methods

#### `Chat.ask(message)`
Basic interaction - send a message and get a response:

```ruby
chat = AgentPk::Chat.new
response = chat.ask("Explain recursion in simple terms")
```

#### `Chat.get(message, schema:, confirm:, out_of:)`
Get a structured response with optional confirmation:

```ruby
# Get a simple answer
answer = chat.get("What is 2 + 2?")

# Get structured response using AgentPk::Schema.result
# This creates a schema that accepts either success or error responses
schema = AgentPk::Schema.result do
  string(:name, description: "Person's name")
  string(:email, description: "Person's email")
end

result = chat.get(
  "Extract the name and email from this text: 'Contact John at john@example.com'",
  schema: schema
)
# => { "status" => "success", "name" => "John", "email" => "john@example.com" }

# If the LLM can't complete the task, it returns an error response:
# => { "status" => "error", "message" => "No email found in the text" }
```

**Using `confirm` and `out_of` for consensus:**

LLMs are non-deterministic and can give different answers to the same question. The `confirm` feature asks the question multiple times and only accepts an answer when it appears at least `confirm` times out of `out_of` attempts. This gives you much higher confidence the answer isn't a hallucination or random variation.

```ruby
class YesOrNoSchema < RubyLLM::Schema
  string(:value, enum: ["yes", "no"])
end

confirmed = chat.get(
  "Is vanilla better than chocolate?",
  confirm: 2,    # Need 2 matching answers
  out_of: 3,      # Out of 3 attempts max
  schema: YesOrNoSchema
)
```

#### `Chat.refine(message, schema:, times:)`
Iteratively refine a response by having the LLM review and improve its own answer.

The refine feature asks your question, gets an answer, then asks the LLM to review that answer for accuracy and improvements. This repeats for the specified number of times. Each iteration gives the LLM a chance to catch mistakes, add detail, or improve quality.

This works because LLMs are often better at *reviewing* content than generating it perfectly the first time - like having an editor review a draft. It's especially effective for creative tasks, complex analysis, or code generation where iterative improvement leads to higher quality outputs.

```ruby
HaikuSchema = RubyLLM::Schema.object(
  haiku: RubyLLM::Schema.string
)

refined_answer = chat.refine(
  "Write a haiku about programming",
  schema: HaikuSchema,
  times: 3  # LLM reviews and refines its answer 3 times
)
```

### Automatic Query Persistence

Every interaction is automatically saved to a SQLite database:

- Full conversation history
- Token usage metrics
- Timestamps and metadata
- Tool calls and responses

```ruby
AgentPk.configure do |config|
  config.db_path = 'tmp/db/agent.sqlite3'
  config.project = 'my_project'
  config.run_id = Time.now.to_i  # Optional: group related queries
end
```

### Cost Reporting

Track your LLM usage and costs:

```ruby
# Generate a cost report for all projects
AgentPk::Reports::Cost.call

# For a specific project
AgentPk::Reports::Cost.call(project: 'my_project')

# For a specific run
AgentPk::Reports::Cost.call(project: 'my_project', run_id: 1234567890)
```

The report includes:
- Input/output token counts
- Cache hit rates
- Per-interaction costs
- Total spending
- Pricing for both normal and long-context models

### Custom Tools

Agent comes with several built-in tools that the LLM can use:

- `dir_glob` - List files matching patterns
- `read_file` - Read file contents
- `edit_file` - Make changes to files
- `file_metadata` - Get file information
- `grep` - Search within files
- `run_rails_test` - Execute Rails tests

```ruby
# Chat with all tools available
chat = AgentPk::Chat.new(tools: AgentPk::Tools.all)

# Or specify specific tools
chat = AgentPk::Chat.new(tools: [:read_file, :grep])
```

## Configuration Options

```ruby
AgentPk.configure do |config|
  config.db_path = 'path/to/database.sqlite3'  # Where to store queries
  config.project = 'project_name'              # Project identifier
  config.run_id = Time.now.to_i                # Optional run grouping
  config.workspace_dir = Dir.pwd               # Working directory for tools
  config.logger = Logger.new($stdout)          # Custom logger
  config.i18n_path = 'path/to/locale.yml'      # Custom translations
end
```

## Requirements

- Ruby >= 3.0.0
- AWS credentials configured for Bedrock access
- SQLite3

## License

WTFPL - Do What The Fuck You Want To Public License

## Author

Pete Kinnecom (git@k7u7.com)
